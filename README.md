# K3s Homelab Cluster

3-Node High-Availability Kubernetes Cluster f√ºr Smart Home & Homelab Services

## üèóÔ∏è Cluster Architektur

### Hardware
- **rpi5** (8GB RAM, NVMe) - `node-role: high-memory` - Monitoring & Memory-intensive Apps
- **rpi4-cm4** (4GB RAM, NVMe) - `node-role: monitoring` - General Workloads
- **rpi4** (4GB RAM, SSD) - `node-role: network-services` - Network Services

Alle Nodes sind Control Plane + Worker (keine Taints)

### Infrastructure
- **K3s** v1.30.8+k3s1
- **MetalLB** v0.14.9 - LoadBalancer (192.168.1.220-239)
- **Longhorn** v1.7.2 - Distributed Storage (2 Replicas)
- **ArgoCD** v2.13.2 - GitOps Continuous Deployment
- **Renovate Bot** - Automatic dependency updates via GitHub PRs

---

## üöÄ Deployed Services

| Service | Type | IP/Port | Description | Node Affinity |
|---------|------|---------|-------------|---------------|
| **Pi-hole v2025** | DNS + Ad-Blocking | 192.168.1.220:80 (Web)<br>192.168.1.225:53 (DNS UDP) | Network-wide ad-blocking | rpi4 (StatefulSet) |
| **Home Assistant v2025** | Smart Home | 192.168.1.223:8123 | Home automation hub | rpi4/rpi5 |
| **Jellyfin** | Media Server | 192.168.1.224:8096 | 2x USB3 HDDs (Movies, Series, Music) | rpi5 (direct USB) |
| **MQTT** | Message Broker | 192.168.1.222:1883 | IoT communication | Any node |
| **Portainer** | Management UI | 192.168.1.227:9443 | Kubernetes Web UI | Any node |
| **Prometheus** | Monitoring | Internal | Metrics collection & storage | rpi5 (high-memory) |
| **Grafana** | Dashboards | 192.168.1.228 | Monitoring dashboards | rpi5 (high-memory) |
| **ArgoCD** | GitOps | Port-Forward 8080 | Continuous deployment | Any node |

---

## üì¶ Deployment

### Via ArgoCD (GitOps) - RECOMMENDED
```bash
# All apps auto-sync from GitHub
kubectl get applications -n argocd

# Applications managed:
# - root-app (manages all other apps)
# - core-apps (Pi-hole, MQTT)
# - home-apps (Home Assistant, Jellyfin)
# - infrastructure (MetalLB, Longhorn)
# - monitoring (Prometheus, Grafana)
```

### Manual Deployment (for troubleshooting)
```bash
# Core Services
kubectl apply -f kubernetes/core/

# Home Apps
kubectl apply -f kubernetes/apps/

# Monitoring
kubectl apply -f kubernetes/monitoring/
```

---

## ü§ñ Automation

### Renovate Bot
- **Automatic Updates**: Scans repository for Docker image updates
- **Pull Requests**: Creates PRs with release notes and changelogs
- **Review & Merge**: Merge PR ‚Üí ArgoCD deploys automatically
- **Dashboard**: https://github.com/LexLandau/homelab-k3s/issues/3

### ArgoCD GitOps
- **Auto-Sync**: Enabled for infrastructure and monitoring
- **Manual Sync**: Core and home apps (for safety)
- **Health Checks**: Monitors application health
- **Rollback**: Easy rollback via Git revert

---

## üîß Useful Commands
```bash
# Cluster Status
kubectl get nodes -L node-role
kubectl get pods -A
kubectl top nodes

# Services & IPs
kubectl get svc -A | grep LoadBalancer

# ArgoCD UI
kubectl port-forward svc/argocd-server -n argocd 8080:443
# https://localhost:8080

# Grafana (Monitoring)
# http://192.168.1.228

# Portainer (Management)
# https://192.168.1.227:9443

# Logs
kubectl logs -n <namespace> <pod-name> -f

# Restart Deployment
kubectl rollout restart deployment <name> -n <namespace>
```

---

## üåê Network Configuration

- **Fritz!Box**: 192.168.1.1 (Router + DNS Upstream)
- **Pi-hole DNS**: 192.168.1.225 (UDP) - Configured in Fritz!Box DHCP
- **MetalLB Pool**: 192.168.1.220-192.168.1.239
- **Cluster Network**: 10.42.0.0/16 (Flannel)
- **Service Network**: 10.43.0.0/16
- **Connection**: Ethernet (eth0) - Migrated from WLAN

---

## üìä Monitoring

### Prometheus + Grafana Stack
```bash
# Grafana Dashboards (pre-imported):
# - 15759: Kubernetes / Compute Resources / Cluster
# - 15760: Kubernetes / Compute Resources / Node  
# - 1860: Node Exporter Full
# - 13032: Longhorn Dashboard (optional)

# Access Grafana:
http://192.168.1.228

# Prometheus Query:
kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090
# http://localhost:9090
```

---

## üì° Network Migration Notes

**Cluster migrated from WLAN to Ethernet (Dec 1, 2025)**

Changes:
- All Pis connected via Ethernet cables (Keller)
- WLAN disabled on all Pis
- K3s flannel interface changed from `wlan0` to `eth0`
- All services running stable on wired network

Performance improvements:
- Lower latency
- More stable connections  
- Better throughput for Longhorn storage
- No WiFi interference

---

## üéØ Best Practices Implemented

### Pi-hole
- ‚úÖ **StatefulSet** instead of Deployment (proper for databases)
- ‚úÖ **NO hostNetwork** (clean networking)
- ‚úÖ **externalTrafficPolicy: Local** (source IP preservation)
- ‚úÖ **PersistentVolumes** via Longhorn (data persistence)

### Monitoring
- ‚úÖ **Node Affinity** - Prometheus on rpi5 (8GB RAM)
- ‚úÖ **Resource Limits** - Controlled memory usage
- ‚úÖ **Persistent Storage** - Metrics retained across restarts

### GitOps
- ‚úÖ **App-of-Apps Pattern** - Centralized management
- ‚úÖ **Automated Updates** - Renovate Bot
- ‚úÖ **Health Checks** - ArgoCD monitors all apps

---

## üîê Secrets Management

Secrets stored in **1Password**:
- ArgoCD Admin Credentials
- Pi-hole Admin Password  
- GitHub Personal Access Token
- K3s Cluster Token
- Grafana Admin Password
- Portainer Admin Password

**‚ö†Ô∏è NEVER commit secrets to Git!**

---

## üìö Future Enhancements

Potential additions:
- [ ] **Traefik** - Reverse proxy with automatic HTTPS
- [ ] **Cert-Manager** - Automatic SSL certificates
- [ ] **Velero** - Kubernetes backup solution
- [ ] **Uptime Kuma** - Service uptime monitoring
- [ ] **ESPHome** - ESP32/ESP8266 firmware management
- [ ] **Nextcloud** - Private cloud storage
- [ ] **ADSB Tracker** - Flight tracking

---

## üéâ Current Status: PRODUCTION-READY!

### Deployed Services:
- ‚úÖ **Pi-hole v2025**: DNS + Ad-Blocking (Best Practice setup)
- ‚úÖ **Home Assistant v2025**: Smart Home automation
- ‚úÖ **Jellyfin**: Media Server with 2x USB3 HDDs
- ‚úÖ **MQTT**: IoT Message Broker
- ‚úÖ **Portainer**: Kubernetes Management UI
- ‚úÖ **Prometheus + Grafana**: Full monitoring stack

### Architecture Highlights:
- üèóÔ∏è **3-Node HA Kubernetes Cluster** on Ethernet
- üîÑ **GitOps** with ArgoCD - All deployments from Git
- ü§ñ **Automated Updates** - Renovate Bot handles dependencies
- üíæ **Distributed Storage** - Longhorn with 2 replicas
- üåê **LoadBalancer** - MetalLB for service IPs
- üìä **Full Observability** - Prometheus + Grafana monitoring
- üõ°Ô∏è **Fault Tolerant** - Can survive 1 node failure (etcd quorum)

### Memory Distribution (Optimized):
- **rpi5 (8GB)**: Prometheus (912MB) + Grafana (691MB) + Jellyfin = 67%
- **rpi4 (4GB)**: Pi-hole + Network services = 66%
- **rpi4-cm4 (4GB)**: General workloads = 57%

All nodes well balanced! ‚úÖ

---

## üìù Maintenance

### Update Workflow:
1. Renovate detects updates ‚Üí Creates PR
2. Review release notes in PR
3. Merge PR in GitHub
4. ArgoCD auto-syncs (3 minutes)
5. Done! ‚ú®

### Backup Strategy:
- **GitOps**: All configs in Git (version controlled)
- **Longhorn**: 2 replicas across nodes
- **Manual**: Regular etcd snapshots on nodes

### Health Checks:
```bash
# Daily check:
kubectl get nodes
kubectl get pods -A | grep -v Running
kubectl top nodes

# Weekly check:
# - Grafana dashboards for trends
# - Renovate dependency dashboard
# - Longhorn volume health
```

---

**Last Updated**: December 2, 2025  
**Cluster Version**: K3s v1.30.8+k3s1  
**Status**: üü¢ PRODUCTION-READY

---

## üîß Recent Optimizations (December 19, 2025)

### Pi-hole Performance Tuning
Resolved DNS performance issues and slow UI saves:

**Problem**: 
- Internetseiten waren sekunden-lang nicht erreichbar
- Pi-hole UI speichern dauerte 10-12 Sekunden
- 365.000 Query-Log-Eintr√§ge belasteten jeden Neustart

**L√∂sung**:
1. **Storage-Optimierung**: Neue `longhorn-fast` StorageClass mit 2 Replicas (statt 3)
   - 33% weniger Netzwerk-Overhead bei Schreibzugriffen
   - Konsistente 60 MB/s Write-Performance
2. **Query-Log-Begrenzung**: `MAXDBDAYS=1` in pihole-FTL.conf
   - Datenbank bleibt unter 100KB (war >100MB)
   - Nur noch ~200 Queries im RAM (war 147.000)
3. **Upstream-DNS optimiert**: Fritz!Box (DNS-over-TLS) mit Google/Cloudflare Fallbacks

**Ergebnis**:
- ‚úÖ UI Save-Zeit: 10-12s ‚Üí **3s** (70% Verbesserung)
- ‚úÖ DNS Cache Hits: **0ms** (vorher "stale answers")
- ‚úÖ Datenbank: 365k Rows ‚Üí **~200 Rows** (99.9% Reduktion)
- ‚úÖ Keine Verbindungsprobleme mehr

**Alle √Ñnderungen sind GitOps-managed** und werden automatisch bei Neudeployments angewendet.

